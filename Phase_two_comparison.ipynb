{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data exploration:\n",
      "                      reviewText     label\n",
      "count                       1000      1000\n",
      "unique                       709         3\n",
      "top     weird smell even washing  Positive\n",
      "freq                           5       334\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('D:\\Sem6\\COMP 262 - NLP and Recommender Systems\\Project\\Final\\df_comparison.csv')\n",
    "\n",
    "print(\"Initial data exploration:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Positive    334\n",
       "Neutral     333\n",
       "Negative    333\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               really cute happy ring\n",
       "1    size 10 dd bought large fit perfectly tight loose\n",
       "2    husband use brand reorder needed using brand year\n",
       "3    great looking fitting cap highly reccomend fin...\n",
       "4    love tee shirt great color thickness fabric sh...\n",
       "Name: reviewText, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['reviewText'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate VADER sentiment\n",
    "def vader_sentiment(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    return analyzer.polarity_scores(text)['compound']\n",
    "\n",
    "def vader_sentiment_label(compound_score):\n",
    "    if compound_score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Function to calculate TextBlob sentiment\n",
    "def textblob_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def textblob_sentiment_label(polarity_score):\n",
    "    if polarity_score > 0:\n",
    "        return 'Positive'\n",
    "    elif polarity_score < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "vectorizer = joblib.load(r'D:\\Sem6\\COMP 262 - NLP and Recommender Systems\\Project\\Final\\pickle files\\tfidf_vectorizer.pkl')\n",
    "lr_model = joblib.load(r'D:\\Sem6\\COMP 262 - NLP and Recommender Systems\\Project\\Final\\pickle files\\logistic_regression_model.pkl')\n",
    "svm_model = joblib.load(r'D:\\Sem6\\COMP 262 - NLP and Recommender Systems\\Project\\Final\\pickle files\\support_vector_machine_model.pkl')\n",
    "nb_model = joblib.load(r'D:\\Sem6\\COMP 262 - NLP and Recommender Systems\\Project\\Final\\pickle files\\naive_bayes_model.pkl')\n",
    "gb_model = joblib.load(r'D:\\Sem6\\COMP 262 - NLP and Recommender Systems\\Project\\Final\\pickle files\\gradient_boosting_model.pkl')\n",
    "mlp_model = joblib.load(r'D:\\Sem6\\COMP 262 - NLP and Recommender Systems\\Project\\Final\\pickle files\\multilayer_perceptron_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8059"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader_sentiment(df['reviewText'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vader_sentiment'] = df['reviewText'].astype('U').apply(vader_sentiment)\n",
    "df['vader_sentiment_label'] = df['vader_sentiment'].apply(vader_sentiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['textblob_sentiment'] = df['reviewText'].astype('U').apply(textblob_sentiment)\n",
    "df['textblob_sentiment_label'] = df['vader_sentiment'].apply(textblob_sentiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.transform(df['reviewText'].astype('U'))\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pred = lr_model.predict(X)\n",
    "svm_pred = svm_model.predict(X)\n",
    "nb_pred = nb_model.predict(X)\n",
    "gb_pred = gb_model.predict(X)\n",
    "mlp_pred = mlp_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gitan\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "lr_accuracy = accuracy_score(y, lr_pred)\n",
    "lr_precision = precision_score(y, lr_pred, average='weighted')\n",
    "lr_recall = recall_score(y, lr_pred, average='weighted')\n",
    "lr_f1 = f1_score(y, lr_pred, average='weighted')\n",
    "\n",
    "svm_accuracy = accuracy_score(y, svm_pred)\n",
    "svm_precision = precision_score(y, svm_pred, average='weighted')\n",
    "svm_recall = recall_score(y, svm_pred, average='weighted')\n",
    "svm_f1 = f1_score(y, svm_pred, average='weighted')\n",
    "\n",
    "nb_accuracy = accuracy_score(y, nb_pred)\n",
    "nb_precision = precision_score(y, nb_pred, average='weighted')\n",
    "nb_recall = recall_score(y, nb_pred, average='weighted')\n",
    "nb_f1 = f1_score(y, nb_pred, average='weighted')\n",
    "\n",
    "gb_accuracy = accuracy_score(y, gb_pred)\n",
    "gb_precision = precision_score(y, gb_pred, average='weighted')\n",
    "gb_recall = recall_score(y, gb_pred, average='weighted')\n",
    "gb_f1 = f1_score(y, gb_pred, average='weighted')\n",
    "\n",
    "mlp_accuracy = accuracy_score(y, mlp_pred)\n",
    "mlp_precision = precision_score(y, mlp_pred, average='weighted')\n",
    "mlp_recall = recall_score(y, mlp_pred, average='weighted')\n",
    "mlp_f1 = f1_score(y, mlp_pred, average='weighted')\n",
    "\n",
    "\n",
    "txt_accuracy = accuracy_score(y, df['textblob_sentiment_label'])\n",
    "txt_precision = precision_score(y, df['textblob_sentiment_label'], average='weighted')\n",
    "txt_recall = recall_score(y, df['textblob_sentiment_label'], average='weighted')\n",
    "txt_f1 = f1_score(y, df['textblob_sentiment_label'], average='weighted')\n",
    "\n",
    "\n",
    "vad_accuracy = accuracy_score(y, df['vader_sentiment_label'])\n",
    "vad_precision = precision_score(y, df['vader_sentiment_label'], average='weighted')\n",
    "vad_recall = recall_score(y, df['vader_sentiment_label'], average='weighted')\n",
    "vad_f1 = f1_score(y, df['vader_sentiment_label'], average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    {'Model': 'Logistic Regression', 'Accuracy': lr_accuracy, 'Precision': lr_precision, 'Recall': lr_recall, 'F1 Score': lr_f1},\n",
    "    {'Model': 'Support Vector Machine', 'Accuracy': svm_accuracy, 'Precision': svm_precision, 'Recall': svm_recall, 'F1 Score': svm_f1},\n",
    "    {'Model': 'Naive Bayes', 'Accuracy': nb_accuracy, 'Precision': nb_precision, 'Recall': nb_recall, 'F1 Score': nb_f1},\n",
    "    {'Model': 'Gradient Boosting', 'Accuracy': gb_accuracy, 'Precision': gb_precision, 'Recall': gb_recall, 'F1 Score': gb_f1},\n",
    "    {'Model': 'Multi-Layer Perceptron', 'Accuracy': mlp_accuracy, 'Precision': mlp_precision, 'Recall': mlp_recall, 'F1 Score': mlp_f1},\n",
    "    {'Model': 'Textblob', 'Accuracy': txt_accuracy, 'Precision': txt_precision, 'Recall': txt_recall, 'F1 Score': txt_f1},\n",
    "    {'Model': 'Vader Sentiment', 'Accuracy': vad_accuracy, 'Precision': vad_precision, 'Recall': vad_recall, 'F1 Score': vad_f1}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------+-------------+----------+------------+\n",
      "| Model                  |   Accuracy |   Precision |   Recall |   F1 Score |\n",
      "|------------------------+------------+-------------+----------+------------|\n",
      "| Logistic Regression    |      0.604 |    0.759071 |    0.604 |   0.541578 |\n",
      "| Support Vector Machine |      0.8   |    0.85317  |    0.8   |   0.796432 |\n",
      "| Naive Bayes            |      0.401 |    0.452567 |    0.401 |   0.28765  |\n",
      "| Gradient Boosting      |      0.622 |    0.773404 |    0.622 |   0.597383 |\n",
      "| Multi-Layer Perceptron |      0.94  |    0.942652 |    0.94  |   0.940112 |\n",
      "| Textblob               |      0.465 |    0.482706 |    0.465 |   0.412106 |\n",
      "| Vader Sentiment        |      0.467 |    0.483455 |    0.467 |   0.415621 |\n",
      "+------------------------+------------+-------------+----------+------------+\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(results, headers='keys', tablefmt='psql'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
